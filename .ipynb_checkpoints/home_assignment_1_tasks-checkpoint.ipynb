{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First home assignment Social Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your solution by email to submission@cssh.rwth-aachen.de until 23.59pm on Wednesday, November 28th!\n",
    "Use \"[SDS] Submission Home Assignment 1\" as the email subject.\n",
    "\n",
    "You can (and should!) submit solutions in teams of up to three members.\n",
    "Please denote all members of the team with their student id and full name in the notebook as well as in the email body. Put all team members in email CC when submitting. Please submit only one email per team.\n",
    "\n",
    "Cite ALL your sources for coding this home assignment. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members will be expelled from the course without warning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement multiple linear regression by gradient descent\n",
    "Your implementation should be based on numpy arrays. Your implementation can use functions from the numpy library.\n",
    "\n",
    "Your function should have the signature\n",
    "linear_regression_gd (X,y,learning_rate) and should return a tuple (mean_squared_error_of_solution, [list_of_optimum_parameters])\n",
    "Here, X is a numpy array with the values of the explanatory variables, and y is an array with the dependent variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Values for X and y from https://www.crashkurs-statistik.de/multiple-lineare-regression/ so we can check if calculations are correct\n",
    "# We also added a row of 1s to X to get the bias term\n",
    "y = np.array([47.1, 46.8, 49.3, 53.2, 47.7, 49.0, 50.6, 47.1, 51.7, 47.8])\n",
    "X = np.array([[156.3, 158.9, 160.8, 179.6, 156.6, 165.1, 165.9, 156.7, 167.8, 160.8], \n",
    "             [62, 52, 83, 69, 74, 52, 77, 65, 79, 51], \n",
    "             [24, 34, 26, 51, 43, 33, 22, 21, 19, 34]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the matrixes\n",
    "# Hacky way with two transpositions to get the bias term\n",
    "\n",
    "\n",
    "X = X.T # transpose so input vectors are along the rows\n",
    "X = np.c_[X, np.ones(X.shape[0])] # add bias term\n",
    "X_Trans = np.array(X) # save the transposed version as X_Trans\n",
    "X = np.array(X.T) # revert to \"original\" X with bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27856669  0.05510842 -0.02139347  0.66299483]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.055353189623654334,\n",
       " array([ 0.27856669,  0.05510842, -0.02139347,  0.66299483]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Following function inspired by:\n",
    "https://stackoverflow.com/questions/11479064/multiple-linear-regression-in-python/33381686\n",
    "\"\"\"\n",
    "#linreg = np.linalg.lstsq(X,y)[0]\n",
    "#print(linreg)\n",
    "\n",
    "# calculate the predictions\n",
    "#print((X.dot(linreg)))\n",
    "\n",
    "#pred = np.dot(X,linreg)\n",
    "\n",
    "#mse = (np.square(y-pred)).mean(axis=0)\n",
    "\n",
    "#print(mse)\n",
    "\n",
    "def hackyRegression(X,y):\n",
    "    linreg = np.linalg.lstsq(X,y)[0]\n",
    "    pred = np.dot(X,linreg)\n",
    "    mse = (np.square(y-pred)).mean(axis=0)\n",
    "    print(linreg)\n",
    "    return(mse, linreg)\n",
    "\n",
    "hackyRegression(X_Trans,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9290842761999332e+20\n",
      "Iteration 0 | Cost: 1.8606830723409102e+40\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,) and (10,) not aligned: 4 (dim 0) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-619057db210e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnumIterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumIterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-619057db210e>\u001b[0m in \u001b[0;36mgradientDescent\u001b[0;34m(X, y, theta, alpha, m, numIterations)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration {0} | Cost: {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_Trans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,) and (10,) not aligned: 4 (dim 0) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "def gradientDescent(X,y,theta,alpha,m,numIterations):\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = sum((np.dot(X, theta)))\n",
    "        print(hypothesis)\n",
    "        loss = hypothesis - y\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        print(\"Iteration {0} | Cost: {1}\".format(i, cost))\n",
    "        for j in range(len(X)):\n",
    "            gradient = (X_Trans[j].dot(loss) / m)\n",
    "            theta = theta - alpha * gradient\n",
    "    return theta\n",
    "\n",
    "n, m = np.shape(X)\n",
    "#print(theta)\n",
    "#print(m, n)\n",
    "numIterations = 100\n",
    "alpha = 0.0001\n",
    "theta = gradientDescent(X,y,theta,alpha,m,numIterations)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[156.3 158.9 160.8 179.6 156.6 165.1 165.9 156.7 167.8 160.8]\n",
      " [ 62.   52.   83.   69.   74.   52.   77.   65.   79.   51. ]\n",
      " [ 24.   34.   26.   51.   43.   33.   22.   21.   19.   34. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]]\n",
      "[[156.3  62.   24.    1. ]\n",
      " [158.9  52.   34.    1. ]\n",
      " [160.8  83.   26.    1. ]\n",
      " [179.6  69.   51.    1. ]\n",
      " [156.6  74.   43.    1. ]\n",
      " [165.1  52.   33.    1. ]\n",
      " [165.9  77.   22.    1. ]\n",
      " [156.7  65.   21.    1. ]\n",
      " [167.8  79.   19.    1. ]\n",
      " [160.8  51.   34.    1. ]]\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_gd(X,y,learning_rate):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend your previous code to implement Stochastic Gradient Descent\n",
    "Your function should have the signature\n",
    "linear_regression_sgd (X,y,learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the quality of Government Dataset \n",
    "from here: https://www.qogdata.pol.gu.se/data/qog_bas_cs_jan18.csv.\n",
    "The data is described here\n",
    "load it into a pandas dataframe and select the following columns:\n",
    "\"cname\",\"wdi_lifexp\",\"wdi_popden\",\"gle_cgdpc\",\"bti_acp\", \"bti_pdi\", \"fh_pair\", \"al_ethnic\",\"al_language\",\"al_religion\",\"bti_aar\",\"vdem_gender\",\"bti_ci\",\"bti_foe\",\"wdi_araland\", \"wdi_forest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://www.qogdata.pol.gu.se/data/qog_bas_cs_jan18.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# select specified columns\n",
    "col = df[[\"cname\",\"wdi_lifexp\",\"wdi_popden\",\"gle_cgdpc\",\"bti_acp\", \"bti_pdi\", \"fh_pair\", \"al_ethnic\",\"al_language\",\"al_religion\",\"bti_aar\",\"vdem_gender\",\"bti_ci\",\"bti_foe\",\"wdi_araland\", \"wdi_forest\"]]\n",
    "col.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the correlation of all other variables with the life expectancy (wdi_lifexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col[col.columns[1:]].corr(\"pearson\")[\"wdi_lifexp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply your own implementations  (GD and SGD)\n",
    "To model the life expectancy from the population density and GDP per capita.\n",
    "Compare the results with what you get from scikit learn OR statsmodel libraries.\n",
    "\n",
    "For this subtask, just remove all countries with missing values in any of these three variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = col.dropna()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build regression models to model the life expectancy (wdi_lifexp) in this dataset\n",
    "from all other mentioned variables.\n",
    "You can use scikit learn and/or statsmodels libraries for this task.\n",
    "Standardize variables and fill in missing values appropriately.\n",
    "Compare linear regression, Ridge regression and Lasso using k-fold-cross validation\n",
    "Test several parameters for the regularized regressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = col.fillna(col.mean(numeric_only=True))\n",
    "df3 = (df3.select_dtypes(exclude=['object']) - df3.mean(numeric_only=True)) / df3.std(numeric_only=True)\n",
    "\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#df.plot.scatter(x=\"vdem_gender\", y=\"wdi_lifexp\")\n",
    "x = df3[[\"wdi_popden\",\"gle_cgdpc\",\"bti_acp\", \"bti_pdi\", \"fh_pair\", \"al_ethnic\",\"al_language\",\"al_religion\",\"bti_aar\",\"vdem_gender\",\"bti_ci\",\"bti_foe\",\"wdi_araland\", \"wdi_forest\"]]\n",
    "y = df3[\"wdi_lifexp\"]\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Lineare Regression\n",
    "linreg = linear_model.LinearRegression()\n",
    "linreg.fit(x,y)\n",
    "print(linreg.coef_)\n",
    "\n",
    "# Ridge Regression\n",
    "ridgereg = linear_model.Ridge(alpha = 1.0)\n",
    "ridgereg.fit(x,y)\n",
    "print(ridgereg.coef_)\n",
    "\n",
    "# Wild-West Regression (Lasso)\n",
    "lassoreg = linear_model.Lasso(alpha=0.1)\n",
    "lassoreg.fit(x,y)\n",
    "print(lassoreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Forward and Backward Selection algorithms\n",
    "And apply it to the (given subset of variables of the) Quality of Government dataset.\n",
    "Compare the results of Forward and Backward selection with each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
